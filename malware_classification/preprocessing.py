import os

import pandas as pd
import skops.io as sio
from pandas import DataFrame
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


def get_dataset() -> (DataFrame, DataFrame):
    with (open('api-call-traces/api_trace.csv', 'r') as csvfile,
          open('api-call-traces/api_trace_labels.txt', 'r') as labels):
        trace = shorten_sequences(csvfile.read().splitlines())
        vectorizer = CountVectorizer(ngram_range=(1, 1), token_pattern=r"(?u)\b\d+\b")
        count_vector = vectorizer.fit_transform(trace)
        sio.dump(vectorizer, os.path.join(os.path.dirname(__file__), 'vectorizer.skops'))

        df = DataFrame(
            data=count_vector.toarray(),
            columns=vectorizer.vocabulary
        )
        df['label'] = labels.read().splitlines()
        return train_test_split(df, test_size=0.3, random_state=43)


def scale_data(x_train: DataFrame, x_test: DataFrame) -> (DataFrame, DataFrame):
    scaler = StandardScaler()
    scaler.fit(pd.concat([x_train, x_test]))
    return scaler.transform(x_train), scaler.transform(x_test)


def shorten_sequences(csvfile: list[str]) -> list[str]:
    """
    Replaces sequences of repeated API calls with a single one.

    :param csvfile: The sequences to modify.
    :return: The new sequences.
    """
    for line in range(len(csvfile)):
        entries = csvfile[line].split(',')
        new_entries = []
        for i in range(1, len(entries)):
            if entries[i] != entries[i - 1]:
                new_entries.append(entries[i])
        csvfile[line] = ','.join(new_entries)
    return csvfile


if __name__ == '__main__':
    train, test = get_dataset()
    print(train.head(5).to_string())